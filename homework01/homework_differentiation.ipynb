{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it easy to google every task please please please try to undestand what's going on. The \"just answer\" thing will be not counted, make sure to present derivation of your solution. It is absolutely OK if you found an answer on web then just exercise in $\\LaTeX$ copying it into here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links: \n",
    "[1](http://www.machinelearning.ru/wiki/images/2/2a/Matrix-Gauss.pdf)\n",
    "[2](http://www.atmos.washington.edu/~dennis/MatrixCalculus.pdf)\n",
    "[3](http://cal.cs.illinois.edu/~johannes/research/matrix%20calculus.pdf)\n",
    "[4](http://research.microsoft.com/en-us/um/people/cmbishop/prml/index.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  \n",
    "y = x^Tx,  \\quad x \\in \\mathbb{R}^N \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dy}{dx} = \n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Solution: $\\frac{d}{dx} (x^Tx) = 2x$ (that's basically a derivative of $x^2$)\n",
    "\n",
    "derivation:\n",
    "\n",
    "$x^Tx = \\sum_{i=1}^N x_i^2$<br><br>\n",
    "$\\frac \\partial {\\partial x_p} \\big [ \\sum_{i=1}^N x_i^2 \\big] = 2x_p$, where $p \\in N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = tr(AB) \\quad A,B \\in \\mathbb{R}^{N \\times N} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dy}{dA} =\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Solution: $\\frac{d}{dA} tr(AB) = B^T$\n",
    "\n",
    "Derivation: $tr(AB) =  \\sum_{i=1}^N \\sum_{k=1}^M a_{ik}b_{ki}$\n",
    "\n",
    "$\\big[ \\frac \\partial {\\partial A} tr(AB) \\big]_{pq} = \\frac \\partial {\\partial a_{pq}} \\big ( \\sum_{i=1}^N \\sum_{k=1}^M a_{ik}b_{ki} \\big ) = b_{qp}$, where $p \\in N, q \\in M$\n",
    "\n",
    "Thus, each element of the result matrix of partial derivatives is an element of the matrix $B$ with row and column index switched or we can simply say it's $B^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  \n",
    "y = x^TAc , \\quad A\\in \\mathbb{R}^{N \\times N}, x\\in \\mathbb{R}^{N}, c\\in \\mathbb{R}^{N} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint for the latter (one of the ways): use *ex. 2* result and the fact \n",
    "$$\n",
    "tr(ABC) = tr (CAB)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " - $\\frac{d}{dx} (x^TAc)= c^TA^T$\n",
    " \n",
    " derivation:\n",
    " $x^TAc = \\sum_{j=1}^N c_j \\sum_{i=1}^N x_i a_{ij} = \\sum_{j=1}^N \\sum_{i=1}^N c_j x_i a_{ij}$\n",
    " \n",
    " $\\frac \\partial {\\partial x} \\big [ \\sum_{j=1}^N \\sum_{i=1}^N c_j x_i a_{ij} \\big]_p = \\sum_{j \\in N} c_j a_{pj} = c^TA^T$, where $p \\in N$<br><br>\n",
    " \n",
    " \n",
    " \n",
    " - $\\frac{d}{dA} (x^TAc) = xc^T$\n",
    " \n",
    " derivation:\n",
    " $x^TAc = \\sum_{j=1}^N c_j \\sum_{i=1}^N x_i a_{ij} = \\sum_{j=1}^N \\sum_{i=1}^N c_j x_i a_{ij}$\n",
    " \n",
    " $\\frac \\partial {\\partial a_{pq}} \\big [ \\sum_{j=1}^N \\sum_{i=1}^N c_j x_i a_{ij} \\big] = c_q x_p = x_p c_q = xc^T$, where $p,q \\in N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classic matrix factorization example. Given matrix $X$ you need to find $A$, $S$ to approximate $X$. This can be done by simple gradient descent iteratively alternating $A$ and $S$ updates.\n",
    "$$\n",
    "J = || X - AS ||_F^2  , \\quad A\\in \\mathbb{R}^{N \\times R} , \\quad S\\in \\mathbb{R}^{R \\times M}\n",
    "$$\n",
    "$$\n",
    "\\frac{dJ}{dS} = ? \n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First approach\n",
    "Using ex.2 and the fact:\n",
    "$$\n",
    "|| X ||_F^2 = tr(XX^T) \n",
    "$$ \n",
    "it is easy to derive gradients (you can find it in one of the refs). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Second approach\n",
    "You can use *slightly different techniques* if they suits you. Take a look at this derivation:\n",
    "<img src=\"grad.png\">\n",
    "(excerpt from [Handbook of blind source separation, Jutten, page 517](https://books.google.ru/books?id=PTbj03bYH6kC&printsec=frontcover&dq=Handbook+of+Blind+Source+Separation&hl=en&sa=X&ved=0ahUKEwi-q_apiJDLAhULvXIKHVXJDWcQ6AEIHDAA#v=onepage&q=Handbook%20of%20Blind%20Source%20Separation&f=false), open for better picture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Third approach\n",
    "And finally we can use chain rule! **YOUR TURN** to do it.\n",
    "let $ F = AS $ \n",
    "\n",
    "**Find**\n",
    "<center>\n",
    "The second approach is a pure madness, so I'll use the first one:<br>\n",
    "\n",
    "$$J = || X - AS ||_F^2 = || X - F ||_F^2 = tr\\big([X-F][X-F]^T\\big) = tr\\big( XX^T - XF^T - FX^T + FF^T \\big) = $$\n",
    "$$=tr\\big( XX^T \\big) - tr\\big( XF^T \\big) - tr\\big( FX^T \\big) + tr\\big( FF^T \\big) = tr\\big( XX^T \\big) - 2tr\\big( XF^T \\big) + tr\\big( FF^T \\big)$$\n",
    "<br><br>\n",
    "$$\n",
    "\\frac{dJ}{dF} = \\frac d {dF} tr\\big( XX^T \\big) - 2 \\frac d {dF} tr\\big( XF^T \\big) + \\frac d {dF} tr\\big( FF^T \\big) = -2X + 2F = -2X+2AS,\\text{from frobenius norm and trace properties and gradients that we derived before}\n",
    "$$\n",
    "<br><br>\n",
    "and \n",
    "$$\n",
    "\\frac{dF}{dS} =  \\frac d {dS} (AS) = \\frac \\partial {\\partial s_{pq}} \\big [ \\sum_{k=1}^R a_{ik}s_{kj} \\big], \\text{where \n",
    "} i \\in N; j,q \\in M; p \\in R\n",
    "$$\n",
    "$$\n",
    "\\frac \\partial {\\partial s_{pq}} \\big [ \\sum_{k=1}^R a_{ik}s_{kj} \\big] = a_{qp} = A^T\n",
    "$$\n",
    "\n",
    "Now it is easy do get desired gradients:\n",
    "$$\n",
    "\\frac{dJ}{dS} =  \\frac {dF} {dS} \\frac {dJ} {dF}  = A^T(-2X+2AS) = 2A^T(AS-X)\n",
    "$$ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
